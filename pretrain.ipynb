{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "658a1d32",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from torch.utils.data import Dataset\n",
    "from utils import generate_vpvr\n",
    "import torch\n",
    "\n",
    "\n",
    "class HourlyDataset(Dataset):\n",
    "    # metadata:\n",
    "    # - vpvr (on prev week)\n",
    "\n",
    "    # - prev weekly high\n",
    "    # - prev weekly low\n",
    "    # - prev weekly open\n",
    "    # - prev weekly close\n",
    "    # - monday open\n",
    "    # - monday high\n",
    "    # - monday low\n",
    "    # - monday close\n",
    "\n",
    "    # - bias vector\n",
    "    # - 2 category (weekend, weekday{\\Monday})\n",
    "    def __init__(self, ltf_data, window_size, split=None):\n",
    "        super(HourlyDataset, self).__init__()\n",
    "        # at the end of init:\n",
    "        # ltf_data:\n",
    "        # ltf_candles:\n",
    "        #    ['open', 'high', 'low', 'close', 'volume'] 'time' 'week_idx' 'usable' 'weekday'\n",
    "        # weekly_data:\n",
    "        #    ['pw_high', 'pw_low', 'pw_open', 'pw_close', 'mon_open', 'mon_high', 'mon_low', 'mon_close', 'vpvr']\n",
    "        # usable_ltf_candles:\n",
    "        #    [\"week_idx\", \"weekday\"]\n",
    "\n",
    "        self.split = split\n",
    "        self.ltf_data = ltf_data.copy()\n",
    "        self.ltf_candles = self.ltf_data[[\"open\", \"high\", \"low\", \"close\", \"volume\", \"time\"]].copy()\n",
    "        self.labels = self.ltf_data[[\"label\", \"value\"]].copy()\n",
    "\n",
    "        self.ltf_candles = self._add_week_idx(self.ltf_candles)\n",
    "        self.weekly_data = self._extract_weekly_info(self.ltf_candles)  # also adds \"week_idx\" to ltf_candles\n",
    "        self.window_size = window_size\n",
    "\n",
    "        self._handle_weekly_none()                      # for weekly\n",
    "        self._mark_usable_or_not()                      # for ltf_candles\n",
    "        self._guarantee_overlap_weekly_and_candles()    # for weekly and ltf_candles\n",
    "\n",
    "        self._control_indices(self.ltf_candles)\n",
    "        self.usable_ltf_candles = self.ltf_candles[self.ltf_candles.usable == 1][[\"week_idx\", \"weekday\"]].copy()\n",
    "        self.usable_ltf_candles = self._handle_consistent_split(self.usable_ltf_candles, self.split)  # handle split\n",
    "\n",
    "        self._drop_unnecessary_and_order_ltf()  # for ltf_candles\n",
    "        self._order_weekly_data()               # for weekly\n",
    "        self.ltf_candles = self.ltf_candles.to_numpy()  # for efficiency\n",
    "\n",
    "    def get_base_datetime(self):\n",
    "        return self.ltf_data.iloc[0].time\n",
    "\n",
    "    def time_price_reaches(self, start_time, price):\n",
    "        subset = self.ltf_data[(self.ltf_data['time'] > start_time) &\n",
    "                               ((self.ltf_data['low'] <= price) &\n",
    "                                (self.ltf_data['high'] >= price))]\n",
    "        return subset.iloc[0].time if not subset.empty else None\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.usable_ltf_candles)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        :return:\n",
    "        x            -> numpy array: window_size * OHLCV\n",
    "        metadata[2:] -> numpy array: ['pw_open', 'pw_close', 'mon_open', 'mon_high', 'mon_low', 'mon_close', 'vpvr']\n",
    "        label        -> numpy array: [direction, value]\n",
    "        weekday      -> int: 1 if weekday, 0 if weekend\n",
    "        \"\"\"\n",
    "        # return candle_seq, metadata (about the week), labels, weekday info\n",
    "        #\n",
    "        last_candle = self.usable_ltf_candles.iloc[idx]\n",
    "        weekday = last_candle.weekday\n",
    "        idx_in_df = last_candle.name\n",
    "        last_candle_datetime = self.ltf_data.loc[idx_in_df].time\n",
    "\n",
    "        # x -> row_idx * OHLCV\n",
    "        x = self.ltf_candles[idx_in_df - (self.window_size - 1): idx_in_df + 1].copy()\n",
    "        weekly_info = self.weekly_data.loc[last_candle.week_idx].to_numpy().copy()\n",
    "        label = self.labels.loc[idx_in_df].to_numpy().copy()\n",
    "\n",
    "        # normalize x and metadata components\n",
    "        x, weekly_info, label, base_info = self._normalize_data_metadata_label(x, weekly_info, label)\n",
    "        base_info.append(last_candle_datetime)\n",
    "\n",
    "        return x, weekly_info[2:], label, weekday, base_info\n",
    "\n",
    "    def _normalize_data_metadata_label(self, x, metadata, label):\n",
    "        # x -> row * OHLCV\n",
    "        # metadata -> series obj with\n",
    "        #     ['pw_high', 'pw_low', 'pw_open', 'pw_close', 'mon_open', 'mon_high', 'mon_low', 'mon_close', 'vpvr']\n",
    "        # label -> label, value\n",
    "\n",
    "        # take the pw_high and pw_low -> index:\n",
    "        #           OHLC of x;\n",
    "        #           'pw_open', 'pw_close', 'mon_open', 'mon_high', 'mon_low', 'mon_close' of metadata;\n",
    "        #           value of label.\n",
    "        pw_high = metadata[0]\n",
    "        pw_low = metadata[1]\n",
    "        pw_mid = (pw_low + pw_high) / 2\n",
    "        pw_half_range = (pw_high - pw_low) / 2\n",
    "\n",
    "        x[:, :4] = (x[:, :4] - pw_mid) / pw_half_range\n",
    "        metadata[2: 8] = (metadata[2: 8] - pw_mid) / pw_half_range\n",
    "        label[1] = (label[1] - pw_mid) / pw_half_range\n",
    "\n",
    "        # standardize the volume\n",
    "        mean_vol = np.mean(x[:, -1])\n",
    "        std_vol = np.std(x[:, -1])\n",
    "        x[:, -1] = (x[:, -1] - mean_vol) / std_vol\n",
    "\n",
    "        return x, metadata, label, [pw_mid, pw_half_range]\n",
    "\n",
    "    def _handle_consistent_split(self, data, split):\n",
    "        # split the data by separating based on the previous week\n",
    "        # 0.7 - 0.2 - 0.1 for train - dev - test\n",
    "        if split is None:\n",
    "            return data\n",
    "\n",
    "        train_prop = 0.7\n",
    "        dev_prop = 0.2\n",
    "        test_prop = 0.1\n",
    "\n",
    "        num_rows = len(data)\n",
    "        train_rows = int(num_rows * train_prop)\n",
    "        dev_rows = int(num_rows * dev_prop)\n",
    "\n",
    "        # Calculate the indices for splitting\n",
    "        train_split_index = train_rows\n",
    "        dev_split_index = train_rows + dev_rows\n",
    "\n",
    "        # Split the DataFrame\n",
    "        if split == \"train\":\n",
    "            train_data = data.iloc[:train_split_index].copy()\n",
    "            return train_data\n",
    "        elif split == \"dev\":\n",
    "            dev_data = data.iloc[train_split_index:dev_split_index].copy()\n",
    "            return dev_data\n",
    "        elif split == \"test\":\n",
    "            test_data = data.iloc[dev_split_index:].copy()\n",
    "            return test_data\n",
    "\n",
    "        assert False, \"Split should be 'train', 'dev', or 'test'\"\n",
    "\n",
    "    \"\"\"\n",
    "    def _handle_consistent_split(self, data, split):\n",
    "        # split the data by separating based on the previous week\n",
    "        # 0.7 - 0.2 - 0.1 for train - dev - test\n",
    "        if split is None:\n",
    "            return data\n",
    "\n",
    "        random_state = 42\n",
    "        train_prop = 0.7\n",
    "        dev_prop = 0.2\n",
    "        test_prop = 0.1\n",
    "\n",
    "        dev_remain = dev_prop / (test_prop + dev_prop)\n",
    "        test_remain = test_prop / (test_prop + dev_prop)\n",
    "\n",
    "        unique_week_indices = pd.Series(data.week_idx.unique())\n",
    "        train_weeks = unique_week_indices.sample(frac=train_prop, random_state=random_state)\n",
    "        if split == \"train\":\n",
    "            return data[data['week_idx'].isin(train_weeks)].copy()\n",
    "\n",
    "        unique_week_indices = unique_week_indices[~unique_week_indices.isin(train_weeks)]\n",
    "        dev_weeks = unique_week_indices.sample(frac=dev_remain, random_state=random_state)\n",
    "        if split == \"dev\":\n",
    "            return data[data['week_idx'].isin(dev_weeks)].copy()\n",
    "\n",
    "        unique_week_indices = unique_week_indices[~unique_week_indices.isin(dev_weeks)]\n",
    "        test_weeks = unique_week_indices\n",
    "        if split == \"test\":\n",
    "            return data[data['week_idx'].isin(test_weeks)].copy()\n",
    "\n",
    "        assert False, \"Split should be 'train', 'dev', or 'test'\"\n",
    "    \n",
    "    \n",
    "        def _handle_consistent_split(self, data, split):\n",
    "        # 0.7 - 0.2 - 0.1 for train - dev - test\n",
    "        if split is None:\n",
    "            return data\n",
    "\n",
    "        random_state = 42\n",
    "        train_prop = 0.7\n",
    "        dev_prop = 0.2\n",
    "        test_prop = 0.1\n",
    "\n",
    "        dev_remain = dev_prop / (test_prop + dev_prop)\n",
    "        test_remain = test_prop / (test_prop + dev_prop)\n",
    "\n",
    "        train_split = data.sample(frac=train_prop, random_state=random_state)\n",
    "        if split == \"train\":\n",
    "            return train_split\n",
    "\n",
    "        train_idx = train_split.index\n",
    "        dev_test_data = data.drop(train_idx, axis=0)\n",
    "        dev_split = dev_test_data.sample(frac=dev_remain, random_state=random_state)\n",
    "        if split == \"dev\":\n",
    "            return dev_split\n",
    "\n",
    "        dev_idx = dev_split.index\n",
    "        test_split = dev_test_data.drop(dev_idx, axis=0)\n",
    "        if split == \"test\":\n",
    "            return test_split\n",
    "\n",
    "        assert False, \"Split should be 'train', 'dev', or 'test'\"\n",
    "    \"\"\"\n",
    "\n",
    "    def _order_weekly_data(self):\n",
    "        order = ['pw_high', 'pw_low', 'pw_open', 'pw_close', 'mon_open', 'mon_high', 'mon_low', 'mon_close', 'vpvr']\n",
    "        self.weekly_data = self.weekly_data[order]\n",
    "\n",
    "    def _drop_unnecessary_and_order_ltf(self):\n",
    "        keep_id = [\"open\", \"high\", \"low\", \"close\", \"volume\"]\n",
    "        for col in self.ltf_candles.columns:\n",
    "            if col not in keep_id:\n",
    "                self.ltf_candles.drop(col, inplace=True, axis=1)\n",
    "\n",
    "        # also order the columns\n",
    "        self.ltf_candles = self.ltf_candles[keep_id]\n",
    "\n",
    "    def _control_indices(self, df):\n",
    "        is_ordered = df.index.equals(pd.RangeIndex(start=0, stop=len(df)))\n",
    "        assert is_ordered, \"The indices must not be corrupted!\"\n",
    "\n",
    "    def _handle_weekly_none(self):\n",
    "        while self.weekly_data.iloc[0].isna().any():\n",
    "            self.weekly_data = self.weekly_data.iloc[1:]\n",
    "\n",
    "        while self.weekly_data.iloc[-1].isna().any():\n",
    "            self.weekly_data = self.weekly_data.iloc[:-1]\n",
    "\n",
    "        if len(self.weekly_data[self.weekly_data.isna().any(axis=1)]) > 0:\n",
    "            none_rows = self.weekly_data[self.weekly_data.isna().any(axis=1)]\n",
    "            print(self.weekly_data[self.weekly_data.isna().any(axis=1)])\n",
    "            assert False, \"Weekly data contains None values!\"\n",
    "\n",
    "    def _guarantee_overlap_weekly_and_candles(self):\n",
    "        # assumes no None value in weekly data\n",
    "        idx_array = self.ltf_candles[self.ltf_candles.usable == 1].week_idx.unique()\n",
    "        weekly_idx_array = self.weekly_data.index.to_numpy()\n",
    "        for idx in idx_array:\n",
    "            if idx not in weekly_idx_array:\n",
    "                print(idx)\n",
    "                assert False, \"Found a non-existing week in weekly data\"\n",
    "\n",
    "    def _mark_usable_or_not(self):\n",
    "        # policy of not usable:\n",
    "        # if monday or among first (window_size - 1) candles, then not usable\n",
    "        # also marks whether weekday or weekend\n",
    "        self.ltf_candles[\"usable\"] = self.ltf_candles.apply \\\n",
    "            (lambda row: 0 if (row.name < (self.window_size - 1) or row[\"time\"].weekday() == 0) else 1, axis=1)\n",
    "        self.ltf_candles[\"weekday\"] = self.ltf_candles.apply(lambda row: 0 if row[\"time\"].weekday() > 4 else 1, axis=1)\n",
    "\n",
    "    def _add_week_idx(self, ltf_data):\n",
    "        # already call by reference but still return it\n",
    "        # add the week_idx column to the original data\n",
    "        # indices are monday date\n",
    "        ltf_data[\"week_idx\"] = (\n",
    "                ltf_data.time - pd.to_timedelta(ltf_data.time.dt.weekday, unit='D')).dt.strftime(\n",
    "            \"%Y-%m-%d\")\n",
    "        return ltf_data\n",
    "\n",
    "    def _extract_weekly_info(self, hourly_candles):\n",
    "        time_idx_candles = hourly_candles.set_index(\"time\", inplace=False)\n",
    "\n",
    "        # Monday ohlc\n",
    "        monday_data = time_idx_candles[time_idx_candles.index.weekday == 0]\n",
    "        monday_ohlc = monday_data.resample(\"W-MON\").agg({\n",
    "            \"open\": \"first\",\n",
    "            \"high\": \"max\",\n",
    "            \"low\": \"min\",\n",
    "            \"close\": \"last\"\n",
    "        })\n",
    "        monday_ohlc.index = monday_ohlc.index.strftime('%Y-%m-%d')\n",
    "        monday_ohlc.columns = [\"mon_open\", \"mon_high\", \"mon_low\", \"mon_close\"]\n",
    "\n",
    "        # handle the case monday data was null\n",
    "        weekday_idx = 1\n",
    "        map_day_str_dict = {1: \"W-TUE\", 2: \"W-WED\", 3: \"W-THU\"}\n",
    "        while len(monday_ohlc[monday_ohlc.isna().any(axis=1)]) > 0:\n",
    "            if weekday_idx == 4:\n",
    "                assert False, \"Problem in Monday data\"\n",
    "\n",
    "            print(f\"Trying to replace {len(monday_ohlc[monday_ohlc.isna().any(axis=1)])} \" +\n",
    "                  f\"monday data (0) with ({weekday_idx})\")\n",
    "            day_data = time_idx_candles[time_idx_candles.index.weekday == weekday_idx]\n",
    "            day_ohlc = day_data.resample(map_day_str_dict[weekday_idx]).agg({\n",
    "                \"open\": \"first\",\n",
    "                \"high\": \"max\",\n",
    "                \"low\": \"min\",\n",
    "                \"close\": \"last\"\n",
    "            })\n",
    "            day_ohlc.index = day_ohlc.index - pd.Timedelta(days=weekday_idx)\n",
    "            day_ohlc.index = day_ohlc.index.strftime('%Y-%m-%d')\n",
    "            day_ohlc.columns = [\"mon_open\", \"mon_high\", \"mon_low\", \"mon_close\"]\n",
    "\n",
    "            null_indices = monday_ohlc[monday_ohlc.isnull().any(axis=1)].index\n",
    "            monday_ohlc.loc[null_indices] = day_ohlc.loc[null_indices]\n",
    "\n",
    "            weekday_idx += 1\n",
    "\n",
    "        # weekly ohlc\n",
    "        weekly_data = time_idx_candles.resample(\"W-SUN\").agg({\n",
    "            \"open\": \"first\",\n",
    "            \"high\": \"max\",\n",
    "            \"low\": \"min\",\n",
    "            \"close\": \"last\"})\n",
    "\n",
    "        # make index monday string\n",
    "        # week indices were already the final day being Sunday\n",
    "        # now we push them by 1 day making it following Monday\n",
    "        weekly_data.index = weekly_data.index + pd.Timedelta(days=1)\n",
    "        assert all(weekly_data.index.dayofweek == 0), \"All dates in weekly_data need to refer to a Monday\"\n",
    "        weekly_data.index = weekly_data.index.strftime(\"%Y-%m-%d\")\n",
    "        weekly_data.columns = [\"pw_open\", \"pw_high\", \"pw_low\", \"pw_close\"]\n",
    "\n",
    "        if (len(weekly_data) - len(monday_ohlc)) > 1:\n",
    "            assert False\n",
    "\n",
    "        weekly_data = pd.merge(weekly_data, monday_ohlc, left_index=True, right_index=True, how='outer')\n",
    "\n",
    "        # VPVR\n",
    "        vpvr_series = pd.Series(dtype=object)\n",
    "        num_bins = 10\n",
    "        for week_idx in time_idx_candles['week_idx'].unique():\n",
    "            # current week\n",
    "            weekly_box = time_idx_candles[time_idx_candles['week_idx'] == week_idx]\n",
    "\n",
    "            if not weekly_box.empty:\n",
    "                buy_array, sell_array = generate_vpvr(weekly_box, num_bins)\n",
    "                # concatenate buy and sell arrays\n",
    "                vpvr_series.at[week_idx] = np.concatenate([buy_array, sell_array])\n",
    "\n",
    "        return pd.merge(weekly_data, vpvr_series.rename(\"vpvr\"), left_index=True, right_index=True, how='outer')\n",
    "\n",
    "    @staticmethod\n",
    "    def get_collate_fn():\n",
    "        def hourly_collate_fn(batch):\n",
    "            x = [item[0] for item in batch]\n",
    "            metadata = [item[1] for item in batch]\n",
    "            label = [item[2] for item in batch]\n",
    "            weekday = [item[3] for item in batch]\n",
    "            base_data = [item[4] for item in batch]\n",
    "\n",
    "            for idx, met in enumerate(metadata):\n",
    "                metadata[idx] = np.concatenate((met[0:-1], met[-1]))\n",
    "\n",
    "            # Convert lists to PyTorch tensors\n",
    "            x_tensor = torch.tensor(x, dtype=torch.float32)  # shape: [BS, seq_len, OHLCV]\n",
    "            metadata_tensor = torch.tensor(metadata, dtype=torch.float32)  # shape: [BS, metadata_dim]\n",
    "            label_tensor = torch.tensor(label, dtype=torch.float32)  # shape: [BS, 2]\n",
    "            weekday_tensor = torch.tensor(weekday, dtype=torch.float32)  # shape: [BS]\n",
    "\n",
    "            return x_tensor, metadata_tensor, label_tensor, weekday_tensor, base_data\n",
    "\n",
    "        return hourly_collate_fn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "75ca8ab4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "\n",
    "\n",
    "class SuperTrainerDataset(Dataset):\n",
    "    def __init__(self):\n",
    "        super(SuperTrainerDataset, self).__init__()\n",
    "        self.length_list = [0]\n",
    "        self.limit_indices = [0]\n",
    "        self.datasets = list()\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        for idx, limit in enumerate(self.limit_indices):\n",
    "            if item < limit:\n",
    "                return self.datasets[idx - 1][item - self.limit_indices[idx - 1]]\n",
    "        assert False, \"Index greater than dataset size\"\n",
    "\n",
    "    def __len__(self):\n",
    "        return sum(self.length_list)\n",
    "\n",
    "    def add_dataset(self, dataset):\n",
    "        self.length_list.append(len(dataset))\n",
    "        self.limit_indices.append(sum(self.length_list))\n",
    "        self.datasets.append(dataset)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3e9d9dd6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 - Loading AUDUSD from data/forex/AUDUSD.csv\n",
      "Building train dataset...\n",
      "Trying to replace 1 monday data (0) with (1)\n",
      "Building dev dataset...\n",
      "Trying to replace 1 monday data (0) with (1)\n",
      "AUDUSD loaded.\n",
      "\n",
      "2 - Loading EURUSD from data/forex/EURUSD.csv\n",
      "Building train dataset...\n",
      "Building dev dataset...\n",
      "EURUSD loaded.\n",
      "\n",
      "3 - Loading GBPUSD from data/forex/GBPUSD.csv\n",
      "Building train dataset...\n",
      "Trying to replace 2 monday data (0) with (1)\n",
      "Building dev dataset...\n",
      "Trying to replace 2 monday data (0) with (1)\n",
      "GBPUSD loaded.\n",
      "\n",
      "4 - Loading NZDUSD from data/forex/NZDUSD.csv\n",
      "Building train dataset...\n",
      "Trying to replace 2 monday data (0) with (1)\n",
      "Building dev dataset...\n",
      "Trying to replace 2 monday data (0) with (1)\n",
      "NZDUSD loaded.\n",
      "\n",
      "5 - Loading USDCAD from data/forex/USDCAD.csv\n",
      "Building train dataset...\n",
      "Trying to replace 2 monday data (0) with (1)\n",
      "Building dev dataset...\n",
      "Trying to replace 2 monday data (0) with (1)\n",
      "USDCAD loaded.\n",
      "\n",
      "6 - Loading USDCHF from data/forex/USDCHF.csv\n",
      "Building train dataset...\n",
      "Trying to replace 2 monday data (0) with (1)\n",
      "Building dev dataset...\n",
      "Trying to replace 2 monday data (0) with (1)\n",
      "USDCHF loaded.\n",
      "\n",
      "7 - Loading USDJPY from data/forex/USDJPY.csv\n",
      "Building train dataset...\n",
      "Trying to replace 1 monday data (0) with (1)\n",
      "Building dev dataset...\n",
      "Trying to replace 1 monday data (0) with (1)\n",
      "USDJPY loaded.\n",
      "\n",
      "8 - Loading EURJPY from data/forex/EURJPY.csv\n",
      "Building train dataset...\n",
      "Trying to replace 1 monday data (0) with (1)\n",
      "Building dev dataset...\n",
      "Trying to replace 1 monday data (0) with (1)\n",
      "EURJPY loaded.\n",
      "\n",
      "9 - Loading GBPJPY from data/forex/GBPJPY.csv\n",
      "Building train dataset...\n",
      "Trying to replace 2 monday data (0) with (1)\n",
      "Building dev dataset...\n",
      "Trying to replace 2 monday data (0) with (1)\n",
      "GBPJPY loaded.\n",
      "\n",
      "10 - Loading EURGBP from data/forex/EURGBP.csv\n",
      "Building train dataset...\n",
      "Trying to replace 1 monday data (0) with (1)\n",
      "Building dev dataset...\n",
      "Trying to replace 1 monday data (0) with (1)\n",
      "EURGBP loaded.\n",
      "\n",
      "11 - Loading AUDJPY from data/forex/AUDJPY.csv\n",
      "Building train dataset...\n",
      "Trying to replace 1 monday data (0) with (1)\n",
      "Building dev dataset...\n",
      "Trying to replace 1 monday data (0) with (1)\n",
      "AUDJPY loaded.\n",
      "\n",
      "12 - Loading USDZAR from data/forex/USDZAR.csv\n",
      "Building train dataset...\n",
      "Trying to replace 2 monday data (0) with (1)\n",
      "Building dev dataset...\n",
      "Trying to replace 2 monday data (0) with (1)\n",
      "USDZAR loaded.\n",
      "\n",
      "13 - Loading EURAUD from data/forex/EURAUD.csv\n",
      "Building train dataset...\n",
      "Trying to replace 1 monday data (0) with (1)\n",
      "Building dev dataset...\n",
      "Trying to replace 1 monday data (0) with (1)\n",
      "EURAUD loaded.\n",
      "\n",
      "14 - Loading GBPAUD from data/forex/GBPAUD.csv\n",
      "Building train dataset...\n",
      "Trying to replace 2 monday data (0) with (1)\n",
      "Building dev dataset...\n",
      "Trying to replace 2 monday data (0) with (1)\n",
      "GBPAUD loaded.\n",
      "\n",
      "15 - Loading NZDJPY from data/forex/NZDJPY.csv\n",
      "Building train dataset...\n",
      "Trying to replace 2 monday data (0) with (1)\n",
      "Building dev dataset...\n",
      "Trying to replace 2 monday data (0) with (1)\n",
      "NZDJPY loaded.\n",
      "\n",
      "16 - Loading EURCHF from data/forex/EURCHF.csv\n",
      "Building train dataset...\n",
      "Trying to replace 1 monday data (0) with (1)\n",
      "Building dev dataset...\n",
      "Trying to replace 1 monday data (0) with (1)\n",
      "EURCHF loaded.\n",
      "\n",
      "17 - Loading EURCAD from data/forex/EURCAD.csv\n",
      "Building train dataset...\n",
      "Trying to replace 2 monday data (0) with (1)\n",
      "Building dev dataset...\n",
      "Trying to replace 2 monday data (0) with (1)\n",
      "EURCAD loaded.\n",
      "\n",
      "18 - Loading GBPCHF from data/forex/GBPCHF.csv\n",
      "Building train dataset...\n",
      "Trying to replace 2 monday data (0) with (1)\n",
      "Building dev dataset...\n",
      "Trying to replace 2 monday data (0) with (1)\n",
      "GBPCHF loaded.\n",
      "\n",
      "19 - Loading GBPCAD from data/forex/GBPCAD.csv\n",
      "Building train dataset...\n",
      "Trying to replace 2 monday data (0) with (1)\n",
      "Building dev dataset...\n",
      "Trying to replace 2 monday data (0) with (1)\n",
      "GBPCAD loaded.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os.path\n",
    "\n",
    "import torch.optim as optim\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from utils import find_last_model, load_data, prepare_data\n",
    "from dataloader.SwingDatasets import HourlyDataset\n",
    "from dataloader.SuperTrainerDataset import SuperTrainerDataset\n",
    "from trainer.trainer import HourlySwingModelTrainer\n",
    "from model.swing_model import HourlySwingModel\n",
    "\n",
    "from dataset_paths import forex_path_dict, crypto_path_dict, index_path_dict, tz_dict\n",
    "\n",
    "pair_types_dict = {\n",
    "    \"forex\": forex_path_dict,\n",
    "    \"crypto\": crypto_path_dict,\n",
    "    \"index\": index_path_dict\n",
    "}\n",
    "\n",
    "with open(\"config_pretrain.json\", \"r\") as file:\n",
    "    config_train = json.load(file)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# train cfg\n",
    "LOAD_MODEL = config_train[\"load_model\"]\n",
    "TYPE = config_train[\"type\"]\n",
    "WINDOW_SIZE = config_train[\"window_size\"]\n",
    "BATCH_SIZE = config_train[\"batch_size\"]\n",
    "VALUE_ONLY = config_train[\"value_only\"]\n",
    "LR = config_train[\"lr\"]\n",
    "EPOCHS = config_train[\"epochs\"]\n",
    "EVAL_PERIOD = config_train[\"eval_period\"]\n",
    "CHECKPOINT_PERIOD = config_train[\"checkpoint_period\"]\n",
    "MODEL_OUT_PATH = config_train[\"model_out\"]\n",
    "\n",
    "# model cfg\n",
    "with open(\"config_model.json\", \"r\") as file:\n",
    "    config_model = json.load(file)\n",
    "\n",
    "inp_dim = config_model[\"inp_dim\"]\n",
    "metadata_dim = config_model[\"metadata_dim\"]\n",
    "metadata_bias = config_model[\"metadata_bias\"]\n",
    "metadata_gate_bias = config_model[\"metadata_gate_bias\"]\n",
    "fusion_model_dim = config_model[\"fusion_model_dim\"]\n",
    "fusion_num_heads = config_model[\"fusion_num_heads\"]\n",
    "fusion_num_layers = config_model[\"fusion_num_layers\"]\n",
    "fusion_apply_grn = config_model[\"fusion_apply_grn\"]\n",
    "fusion_dropout = config_model[\"fusion_dropout\"]\n",
    "lstm_num_layers = config_model[\"lstm_num_layers\"]\n",
    "lstm_bidirectional = config_model[\"lstm_bidirectional\"]\n",
    "lstm_dropout = config_model[\"lstm_dropout\"]\n",
    "loss_punish_cert = config_model[\"loss_punish_cert\"]\n",
    "\n",
    "# load model\n",
    "model = HourlySwingModel(inp_dim=inp_dim, metadata_dim=metadata_dim, metadata_bias=metadata_bias,\n",
    "                         metadata_gate_bias=metadata_gate_bias, fusion_model_dim=fusion_model_dim,\n",
    "                         fusion_num_heads=fusion_num_heads, fusion_num_layers=fusion_num_layers,\n",
    "                         fusion_apply_grn=fusion_apply_grn, fusion_dropout=fusion_dropout,\n",
    "                         lstm_num_layers=lstm_num_layers, lstm_bidirectional=lstm_bidirectional,\n",
    "                         lstm_dropout=lstm_dropout, loss_punish_cert=loss_punish_cert)\n",
    "model.to(device)\n",
    "if LOAD_MODEL is not None:\n",
    "    model_file = find_last_model(LOAD_MODEL)\n",
    "    print(f\"Loading model from: {os.path.join(LOAD_MODEL, model_file)}\")\n",
    "    model.load_state_dict(torch.load(os.path.join(LOAD_MODEL, model_file), map_location=device))\n",
    "\n",
    "# load data and prepare super_dataset\n",
    "pair_path_dict = pair_types_dict[TYPE]\n",
    "super_train_dataset = SuperTrainerDataset()\n",
    "super_dev_dataset = SuperTrainerDataset()\n",
    "idx = 0\n",
    "for pair, path in pair_path_dict.items():\n",
    "    idx += 1\n",
    "    print(f\"{idx} - Loading {pair.upper()} from {path}\")\n",
    "    # load data\n",
    "    pair_data = load_data(path, add_zigzag_col=True)\n",
    "    pair_data = prepare_data(pair_data, tz_dict[pair])\n",
    "    # prepare dataset\n",
    "    print(f\"Building train dataset...\")\n",
    "    train_dataset = HourlyDataset(pair_data, WINDOW_SIZE, \"train\")\n",
    "    print(f\"Building dev dataset...\")\n",
    "    dev_dataset = HourlyDataset(pair_data, WINDOW_SIZE, \"dev\")\n",
    "    # add to super dataset\n",
    "    super_train_dataset.add_dataset(train_dataset)\n",
    "    super_dev_dataset.add_dataset(dev_dataset)\n",
    "    print(f\"{pair.upper()} loaded.\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f4127189",
   "metadata": {},
   "outputs": [],
   "source": [
    "summ = 0\n",
    "for dataset in super_train_dataset.datasets:\n",
    "    summ += len(dataset.ltf_data[dataset.ltf_data.zigzag!=0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c21756e6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "23054"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "64395b53",
   "metadata": {},
   "outputs": [],
   "source": [
    "BatchSize = 3\n",
    "metadata_dim = 4\n",
    "seq_len = 5\n",
    "\n",
    "input_tensor = torch.randn(BatchSize, metadata_dim)  # Replace with your tensor\n",
    "\n",
    "# Expand to the desired shape\n",
    "output_tensor = input_tensor.unsqueeze(1).expand(-1, seq_len, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b7fecd22",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-8.3414e-01, -8.8846e-01, -1.5769e+00,  7.2267e-01],\n",
       "        [-1.2260e+00,  4.3148e-04,  9.7815e-01,  7.1238e-02],\n",
       "        [-4.9249e-01, -2.4486e+00,  3.6938e-02,  2.4355e+00]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d5c4f95b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 4])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_tensor[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "099ef593",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "3089e402",
   "metadata": {},
   "outputs": [],
   "source": [
    "par = nn.Parameter(torch.randn((5, 10)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "2947c7b1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[ 1.3859e+00,  2.7784e-01,  1.0979e-02, -7.9550e-02, -2.4466e-01,\n",
       "         -1.6264e-01,  2.2212e+00, -1.7096e-01, -1.1046e+00, -3.2281e-01],\n",
       "        [-1.3282e+00,  1.4145e+00,  4.7734e-01,  3.2597e-01, -7.9155e-01,\n",
       "         -1.9237e+00,  1.4716e+00, -7.7554e-01, -8.3920e-01,  1.8074e-01],\n",
       "        [-2.2318e-01, -4.0131e-01, -1.1018e+00, -3.5627e-01, -9.2998e-01,\n",
       "         -1.0346e+00, -1.0416e+00, -2.1275e-01,  1.2484e+00,  6.3164e-04],\n",
       "        [-2.0751e-01, -8.2800e-01, -1.0970e+00,  1.4049e+00, -5.3343e-01,\n",
       "          7.3585e-02, -6.6443e-01, -6.6012e-02, -6.3939e-01, -1.2284e+00],\n",
       "        [-7.6871e-01, -1.5211e-01,  6.6235e-01, -1.2609e+00,  7.2886e-01,\n",
       "          2.8504e-01, -8.5848e-01,  2.8860e-01, -4.8599e-01, -6.1851e-01]],\n",
       "       requires_grad=True)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "par"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6b778996",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.randn((3,5,10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8f4d1e6d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 1.4173e-01,  6.5189e-01, -6.6535e-01, -2.1323e-01,  1.0572e+00,\n",
       "           1.5039e+00,  6.4828e-01, -1.8274e-01, -1.5906e-01,  1.2856e+00],\n",
       "         [-1.6971e+00,  1.3763e+00, -1.3518e+00,  3.3029e-01, -4.1408e-01,\n",
       "           3.7386e-01,  6.9113e-03,  9.8356e-01,  8.7109e-01, -5.6164e-01],\n",
       "         [-2.6846e+00, -1.1234e+00,  2.7144e-01, -8.2181e-01, -1.9523e-01,\n",
       "           8.7756e-01, -3.6374e-01, -4.9087e-01,  9.1444e-03, -1.1149e-01],\n",
       "         [-1.1559e+00,  1.7282e+00, -7.9126e-01, -8.4900e-02, -4.4864e-01,\n",
       "          -2.1610e-03, -1.2869e+00, -3.3090e-01, -4.0983e-01, -6.6984e-01],\n",
       "         [ 1.1939e+00,  1.6781e+00, -6.2555e-01,  5.3702e-01, -8.5439e-01,\n",
       "          -6.0880e-01,  1.3065e-01,  3.4493e-01,  4.5945e-01, -4.7505e-01]],\n",
       "\n",
       "        [[-1.6285e+00,  4.0523e-01, -1.6392e+00,  7.1000e-02, -8.4359e-02,\n",
       "          -2.5679e-02, -1.3094e+00,  9.1601e-01,  2.9487e-01,  6.9084e-01],\n",
       "         [-5.7770e-01,  1.4168e+00, -1.3110e-01,  1.3901e+00,  2.0432e+00,\n",
       "          -8.1189e-01, -7.1406e-01,  6.9289e-01,  1.5750e+00, -7.7509e-01],\n",
       "         [-1.2974e+00, -1.0122e+00,  1.8136e+00,  7.7802e-01, -3.8133e-01,\n",
       "           6.5394e-01,  2.8281e+00, -3.5949e-01,  4.1792e-01,  5.5782e-01],\n",
       "         [-1.2459e+00, -9.1789e-01,  1.3924e+00, -3.8399e-01, -1.8311e+00,\n",
       "          -2.7427e-01, -1.2913e+00, -4.6147e-01,  1.0733e+00,  1.3847e+00],\n",
       "         [-4.4543e-01,  8.3710e-01, -7.0973e-01,  7.5644e-02,  2.0661e+00,\n",
       "           7.3545e-01,  1.8009e-02, -5.4630e-01, -7.1294e-01,  9.4044e-01]],\n",
       "\n",
       "        [[-1.2958e+00, -5.1045e-01,  7.3374e-01, -3.5555e-01, -1.0685e-02,\n",
       "          -1.7442e+00,  3.6547e-01,  9.8604e-01, -5.1762e-01,  2.9671e-01],\n",
       "         [ 2.0079e+00, -9.2228e-01, -8.3522e-01, -1.1727e+00,  5.2566e-01,\n",
       "          -9.9102e-02,  1.8924e+00,  1.2796e-01,  2.5854e-01,  2.4207e-01],\n",
       "         [-1.8434e+00,  1.6340e+00, -1.0584e+00, -1.3684e+00,  8.8200e-01,\n",
       "           1.2110e+00,  5.2679e-01,  6.6629e-02,  1.3154e+00, -2.6691e-01],\n",
       "         [-6.6717e-01, -9.0678e-01, -8.4401e-01, -9.6790e-01, -1.7022e-01,\n",
       "           4.1029e-01, -2.1461e+00,  3.3939e-01,  2.6372e-01, -6.5775e-01],\n",
       "         [-1.0803e-01, -4.6540e-01, -5.9567e-01, -7.1893e-01,  5.8266e-01,\n",
       "          -1.7372e+00, -1.1927e-01, -6.7741e-01, -7.1192e-01,  4.5864e-01]]])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d96d2649",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 1.5277,  0.9297, -0.6544, -0.2928,  0.8125,  1.3412,  2.8695,\n",
       "          -0.3537, -1.2636,  0.9628],\n",
       "         [-3.0253,  2.7909, -0.8745,  0.6563, -1.2056, -1.5498,  1.4785,\n",
       "           0.2080,  0.0319, -0.3809],\n",
       "         [-2.9078, -1.5247, -0.8304, -1.1781, -1.1252, -0.1570, -1.4053,\n",
       "          -0.7036,  1.2576, -0.1109],\n",
       "         [-1.3634,  0.9002, -1.8883,  1.3200, -0.9821,  0.0714, -1.9514,\n",
       "          -0.3969, -1.0492, -1.8983],\n",
       "         [ 0.4252,  1.5260,  0.0368, -0.7238, -0.1255, -0.3238, -0.7278,\n",
       "           0.6335, -0.0265, -1.0936]],\n",
       "\n",
       "        [[-0.2426,  0.6831, -1.6282, -0.0085, -0.3290, -0.1883,  0.9118,\n",
       "           0.7451, -0.8097,  0.3680],\n",
       "         [-1.9059,  2.8313,  0.3462,  1.7161,  1.2517, -2.7356,  0.7576,\n",
       "          -0.0826,  0.7358, -0.5943],\n",
       "         [-1.5206, -1.4135,  0.7118,  0.4218, -1.3113, -0.3806,  1.7865,\n",
       "          -0.5722,  1.6663,  0.5585],\n",
       "         [-1.4534, -1.7459,  0.2954,  1.0209, -2.3645, -0.2007, -1.9557,\n",
       "          -0.5275,  0.4339,  0.1563],\n",
       "         [-1.2141,  0.6850, -0.0474, -1.1852,  2.7950,  1.0205, -0.8405,\n",
       "          -0.2577, -1.1989,  0.3219]],\n",
       "\n",
       "        [[ 0.0902, -0.2326,  0.7447, -0.4351, -0.2553, -1.9068,  2.5867,\n",
       "           0.8151, -1.6222, -0.0261],\n",
       "         [ 0.6797,  0.4922, -0.3579, -0.8467, -0.2659, -2.0228,  3.3640,\n",
       "          -0.6476, -0.5807,  0.4228],\n",
       "         [-2.0666,  1.2327, -2.1602, -1.7247, -0.0480,  0.1764, -0.5148,\n",
       "          -0.1461,  2.5638, -0.2663],\n",
       "         [-0.8747, -1.7348, -1.9410,  0.4370, -0.7036,  0.4839, -2.8105,\n",
       "           0.2734, -0.3757, -1.8862],\n",
       "         [-0.8767, -0.6175,  0.0667, -1.9798,  1.3115, -1.4521, -0.9778,\n",
       "          -0.3888, -1.1979, -0.1599]]], grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a + par"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "9ec7a47f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MetricDataloader(DataLoader):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        # Calculate start and end indices for the batch\n",
    "        start_idx = index * self.batch_size\n",
    "        end_idx = start_idx + self.batch_size\n",
    "\n",
    "        batch = [self.dataset[i] for i in range(start_idx, min(end_idx, len(self.dataset)))]\n",
    "\n",
    "        if self.collate_fn is not None:\n",
    "            batch = self.collate_fn(batch)\n",
    "\n",
    "        return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "7de6c20b",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = MetricDataloader(train_dataset, batch_size=1, shuffle=False,\n",
    "                              collate_fn=HourlyDataset.get_collate_fn())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "7820d9c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1.70511,\n",
       " 0.043810000000000016,\n",
       " Timestamp('2020-04-03 04:00:00+0200', tz='Etc/GMT-2')]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataloader[1004][-1][-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64c8735a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
